{
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MNIST in pytorch with Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.stats\n",
        "import scipy.special\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "from matplotlib import cm\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "## Standard boilerplate to import torch and torch related modules\n",
        "import torch\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [*MNIST* dataset](https://en.wikipedia.org/wiki/MNIST_database) is one of the classic datasets in Machine Learning and is often one of the first datasets against which new classification algorithms test themselves.  It consists of 70,000 images of handwritten digits, each of which is 28x28 pixels. You will be using PyTorch to build a handwritten digit classifier that you will train, validate, and test with MNIST. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Regression Parent Class\n",
        "class Regression(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.params = dict()\n",
        "    \n",
        "    def get_params(self, k):\n",
        "        return self.params.get(k, None)\n",
        "    \n",
        "    def set_params(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            self.params[k] = v\n",
        "        \n",
        "                    \n",
        "    def fit(self, X, y):\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def predict(self, X):\n",
        "        raise NotImplementedError()\n",
        "        \n",
        "    def score(self, X, y):\n",
        "        raise NotImplementedError()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(validation_split=10000, batch_size=256):\n",
        "        \"\"\"load the MNIST training and test sets from MNIST\"\"\"\n",
        "        \n",
        "        \n",
        "        ## We start by defining our training dataset\n",
        "        ## --root-- a string pointing to the relative path of the directory where we'll store our MNIST data\n",
        "        ## --train-- tells us whether to download the training set (True) or the test set (False)\n",
        "        ## MNIST in torchvision only has train (60K) and test (10K) datasets.  Other datasets also have a validation set\n",
        "        ## --transforms-- is a torchvision.transforms object that specifies what transforms to apply to each element\n",
        "        ## in the dataset.  The required transform is transforms.ToTensor() that turns each element into a PyTorch floating\n",
        "        ## point tensor object.  You could also add others like transforms.Normalize if you wished\n",
        "        ## --download-- specifies whether to download the data from the online urls.   If set to false, then you should\n",
        "        ## provide the data locally yourself\n",
        "        train_dataset = datasets.MNIST(root='./data',\n",
        "                                    train=True,\n",
        "                                    transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                                     ]),\n",
        "                                    download=True)\n",
        "\n",
        "        ## similar to the above, the main difference is that we should set train=False since we want the\n",
        "\n",
        "        ## test set data\n",
        "        test_dataset = datasets.MNIST(root='./data',\n",
        "                                   train=False,\n",
        "                                   transform=transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.1307,), (0.3081,)),\n",
        "                                     ]),\n",
        "                                   download=True)\n",
        "\n",
        "        ## A DataLoader or (Dataset Loader) turns the specified data set into a sequence of data elements\n",
        "        ## that you can access in your loops for training or evaluating accuracy, etc.\n",
        "\n",
        "        ## First we need to further split our training dataset into training and validation sets.\n",
        "\n",
        "        # Define the indices\n",
        "        indices = list(range(len(train_dataset))) # start with all the indices in training set\n",
        "\n",
        "        # Define your batch_size\n",
        "        batch_size = batch_size\n",
        "\n",
        "        # Random, non-contiguous split\n",
        "        validation_idx = np.random.choice(indices, size=validation_split, replace=False)\n",
        "        train_idx = list(set(indices) - set(validation_idx))\n",
        "\n",
        "        # define our samplers -- we use a SubsetRandomSampler because it will return\n",
        "        # a random subset of the split defined by the given indices without replacement\n",
        "        train_sampler = SubsetRandomSampler(train_idx)\n",
        "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
        "\n",
        "        # Create the train_loader -- use your real batch_size which you\n",
        "        # I hope have defined somewhere above\n",
        "        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                        batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "        # You can use your above batch_size or just set it to 1 here.  Your validation\n",
        "        # operations shouldn't be computationally intensive or require batching.\n",
        "        validation_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                        batch_size=validation_split, sampler=validation_sampler)\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                                  batch_size=len(test_dataset),\n",
        "                                                  shuffle=False)\n",
        "\n",
        "        return (train_dataset, test_dataset, \n",
        "                train_loader, test_loader, \n",
        "                validation_loader,\n",
        "                train_idx, validation_idx\n",
        "               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset, test_dataset, train_loader, test_loader, validation_loader,train_idx, validation_idx = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Stolen from excellent visualization from submission from Madeleine Duran/Sarah Walker\n",
        "def viz_training_loss(training_losses):\n",
        "    \"\"\"Visualize/Plot our training loss\"\"\"\n",
        "    epochs = len(training_losses)\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=epochs, figsize=(20,5), sharex=True, sharey=True)\n",
        "    plt.suptitle(\"Loss Trajectory for MNIST LR Model\", fontsize=20, weight='heavy')\n",
        "    for i in range(epochs):\n",
        "        axes[i].plot(range(len(training_losses[i])), training_losses[i])\n",
        "        axes[i].set_title(\"epoch {}\".format(i))\n",
        "        if i % 2 == 1:\n",
        "            axes[i].axvspan(0, len(training_losses[i]), facecolor='gray', alpha=0.2)\n",
        "    plt.subplots_adjust(wspace=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The multilayer perceptron can be understood as a logistic regression classifier in which the input is first transformed using a learnt non-linear transformation. The non-linear transformation is often chosen to be either the logistic function or the $\\tanh$ function or the RELU function, and its purpose is to project the data into a space where it becomes linearly separable. The output of this so-called hidden layer is then passed to the logistic regression graph that we have constructed in the first problem. \n",
        "\n",
        "![](http://deeplearning.net/tutorial/_images/mlp.png)\n",
        "\n",
        "We'll construct a model with **1 hidden layer**. That is, you will have an input layer, then a hidden layer with the nonlinearity, and finally an output layer with cross-entropy loss (or equivalently log-softmax activation with a negative log likelihood loss).\n",
        "\n",
        "2.1. Using a similar architecture as in Question 1 and the same training, validation and test sets, build a PyTorch model for the multilayer perceptron. Use the $\\tanh$ function as the non-linear activation function. \n",
        "\n",
        "2.2. The initialization of the weights matrix for the hidden layer must assure that the units (neurons) of the perceptron operate in a regime where information gets propagated. For the $\\tanh$ function, you may find it advisable to initialize with the interval $\\left[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}\\right]$, where $fan_{in}$ is the number of units in the $(i-1)$-th layer, and $fan_{out}$ is the number of units in the i-th layer.  This is known as **Xavier Initialization**.  Use Xavier Initialization to initialize your MLP.  Feel free to use PyTorch's in-built Xavier Initialization methods.\n",
        "\n",
        "2.3. Using $\\lambda = 0.01$ to compare with Question 1, experiment with the learning rate (try 0.1 and 0.01 for example), batch size (use 64, 128 and 256) and the number of units in your hidden layer (use between 25 and 200 units). For what combination of these parameters do you obtain the highest validation accuracy?  You may want to start with 20 epochs for running time and experiment a bit to make sure that your models reach convergence. \n",
        "\n",
        "2.4. For your best combination plot the cross-entropy loss on the training set as a function of iteration.\n",
        "\n",
        "2.5. For your best combination use classification accuracy to evaluate how well your model is performing on the validation set at the end of each epoch. Plot this validation accuracy as the model trains.\n",
        "\n",
        "2.6. Select what you consider the best set of parameters and predict the labels of the test set. Compare your predictions with the given labels. What classification accuracy do you obtain on the training and test sets?\n",
        "\n",
        "2.7. How does your test accuracy compare to that of the logistic regression classifier in Question 1?  Compare best parameters for both models.\n",
        "\n",
        "2.8. What classes are most likely to be misclassified? Plot some misclassified training and test set images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class ANN(Regression):\n",
        "    \n",
        "    def __init__(self, input_model, reg_rate = 0.01, learning_rate=0.1, batch_size=256, epochs=30, hidden=None):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        \n",
        "        ## Load MNIST Data\n",
        "        #train_dataset, test_dataset, train_loader, test_loader, validation_loader, train_idx, validation_idx = self.load_data(batch_size=batch_size)\n",
        "        \n",
        "        ## Add Datasets and Data Loaders to our params\n",
        "        self.set_params(train_dataset=train_dataset, \n",
        "                        train_loader=train_loader,\n",
        "                        test_dataset=test_dataset,\n",
        "                        test_loader=test_loader,\n",
        "                        validation_loader=validation_loader,\n",
        "                        train_idx=train_idx,\n",
        "                        validation_idx=validation_idx\n",
        "                       )\n",
        "        \n",
        "        \n",
        "        ## Here we instantiate the PyTorch model that we so nicely defined previously\n",
        "        if hidden == None:\n",
        "            model = input_model()\n",
        "        else:\n",
        "            model = input_model(hidden=hidden)\n",
        "\n",
        "        ## Here we define our loss function.  We're using CrossEntropyLoss but other options include\n",
        "        ## NLLLoss (negative log likelihood loss for when the log_softmax activation is explicitly defined\n",
        "        ## on the output layer), MSELoss for OLS Regression, KLLDivLoss for KL Divergence, BCELoss\n",
        "        ## for binary cross entropy and many others\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        ## Here we define our optimizer.  In class we've been using SGD although in practice one will often\n",
        "        ## use other optimizers like Adam or RMSProp.  The primary parameter the optimizer takes is the\n",
        "        ## set of parameters in your model.  Fortunately those are easily accessible via model.paramters()\n",
        "        ## where model is the instance of the model you defined.  Other useful parameters include lr for the\n",
        "        ## learning rate and weight_decay for the rate of l2 regularization.\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=reg_rate)\n",
        "        \n",
        "        \n",
        "        ## Set the rest of our parameters -- batch_size, learning_rate, epochs, optimizer,\n",
        "        ## model and criterion\n",
        "        \n",
        "        ## Add Datasets and Data Loaders to our params\n",
        "        self.set_params(optimizer=optimizer, \n",
        "                        learning_rate=learning_rate,\n",
        "                        weight_decay=reg_rate,\n",
        "                        batch_size=batch_size,\n",
        "                        model=model,\n",
        "                        criterion=criterion,\n",
        "                        epochs=epochs)   \n",
        "    \n",
        "\n",
        "        \n",
        "    def get_loader(self, dataset):\n",
        "        \"\"\"Retrieve dataloader, images, labels based upon dataset name\"\"\"\n",
        "        \n",
        "        if dataset == 'Test':\n",
        "            loader = self.get_params('test_loader')\n",
        "        elif dataset == 'Validation':\n",
        "            loader = self.get_params('validation_loader')\n",
        "        else:\n",
        "            loader = self.get_params('train_loader')\n",
        "            \n",
        "        # Get Loader\n",
        "        return loader\n",
        "    \n",
        "    def predict(self, dataset='Test'):\n",
        "        \"\"\"Classify images based on the fitted logistic regression model\"\"\"\n",
        "\n",
        "        loader = self.get_loader(dataset)\n",
        "        \n",
        "        predictions = []\n",
        "        all_labels = []\n",
        "        misclassified = []\n",
        "        misclassified_images = []\n",
        "        misclassified_labels = []\n",
        "        misclassified_preds = np.array([])\n",
        "        correct = 0\n",
        "        model = self.get_params('model')\n",
        "\n",
        "        for inputs, labels in loader:\n",
        "\n",
        "            ## get the inputs from the dataloader and turn into a variable for \n",
        "            ## feeding to the model\n",
        "            inputs = Variable(inputs)\n",
        "\n",
        "            ## Reshape so that batches work properly\n",
        "            inputs = inputs.view(-1, 28*28)\n",
        "\n",
        "            # run our model on the inputs\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # get the class of the max log-probability\n",
        "            pred = outputs.data.max(1)[1]\n",
        "            \n",
        "            # get the correct predictions\n",
        "            correct += (pred == labels).sum()\n",
        "\n",
        "            # save current batch of predictions\n",
        "            predictions += list(pred)\n",
        "            \n",
        "            # save all labels\n",
        "            all_labels += list(labels)\n",
        "                  \n",
        "        return np.array(predictions), correct.item()\n",
        "    \n",
        "    \n",
        "    def score(self, dataset='Test', print_score=True):\n",
        "        \"\"\"Calculate accuracy score based upon model classification\"\"\"\n",
        "        \n",
        "        preds, correct = self.predict(dataset=dataset)\n",
        "        #correct = self.get_params('correct_predictions')\n",
        "        total = len(preds)\n",
        "        \n",
        "        if print_score:\n",
        "            print('Dataset: {} \\nAccuracy: {}/{} ({:.1f}%)\\n'.format(\n",
        "                dataset, correct, total, 100.0 * correct / total))\n",
        "        #print(\">>>\", correct/total, type(correct), type(total))\n",
        "        return correct/total\n",
        "    \n",
        "        \n",
        "    def fit(self, do_validation=True, show_validation=True):\n",
        "        \"\"\"Fit our logistic regression model on MNIST training set\"\"\"\n",
        "        \n",
        "        ## We defined a number of variables in our constructor -- let's reclaim them here\n",
        "        optimizer=self.get_params(\"optimizer\")\n",
        "        model=self.get_params(\"model\")\n",
        "        epochs=self.get_params(\"epochs\")\n",
        "        criterion=self.get_params(\"criterion\")\n",
        "        train_loader=self.get_params(\"train_loader\")\n",
        "        \n",
        "        ## Get the Total size of training set\n",
        "        self.get_params('train_dataset')\n",
        "        training_size = len(self.get_params('train_idx'))\n",
        "        \n",
        "        iterations = int(np.ceil(training_size/self.get_params(\"batch_size\")))\n",
        "        \n",
        "        ## We need something to keep track of our losses\n",
        "        losses = np.zeros((epochs, iterations))\n",
        "        \n",
        "        ## We need something ot keep track of our validation scores\n",
        "        validation_scores = np.zeros(epochs)\n",
        "  \n",
        "        \n",
        "        ## Our training loop.  We can loop over a fixed number of epochs or\n",
        "        ## using a sensitivity parameter (i.e. until net change in loss is\n",
        "        ## below a certain tolerance).  Here we iterate over a fixed number of\n",
        "        ## epochs\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            ## We defined our train_loader DataLoader earlier.  The train_loader is a\n",
        "            ## sequence of tuples with the first element of each tuple being\n",
        "            ## the batched training inputs (the batch_size being defined in your DataLoader)\n",
        "            ## and the second second element of each tuple being the corresponding labels\n",
        "            ## more or less all the pytorch classes are built to handle batching transparently\n",
        "\n",
        "            ## loop through the DataLoader.  Each loop is one iteration.  All the loops\n",
        "            ## form one epoch\n",
        "            for batch_index, (inputs, labels) in enumerate(train_loader):\n",
        "\n",
        "                # Convert the inputs/labels passed from the DataLoader into\n",
        "                # autograd Variables.  The dataloader provides them as PyTorch Tensors\n",
        "                # per the transforms.ToTensor() operation.\n",
        "                inputs, labels = Variable(inputs), Variable(labels)\n",
        "\n",
        "                ## as mentioned above we receive the inputs as tensors of size (batch_size,1, 28, 28)\n",
        "                ## which is effectively (batch_size, 28, 28) basically as a 3 dimensional tensor\n",
        "                ## representing a stack of (28x28) matrices with each matrix element a floating point number\n",
        "                ## representing the value of that pixel in the image.  Unfortunately our Neural Network model\n",
        "                ## can't handle that representation and needs a pixel matrices to be flattened into a row vector\n",
        "                ## of inputs.  The model takes a 2d tensor representing batch of such row vectors each row vector\n",
        "                ## representing one set of inputs corresponding to one image.  In order to accomplish this\n",
        "                ## flattening we use the .view method defined on autograd Variables.\n",
        "                inputs = inputs.view(-1, 28*28)\n",
        "\n",
        "                # we need to zero out our gradients after each pass\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "\n",
        "                ## This is the optimize - forward step - backwards step part of our design pattern\n",
        "\n",
        "                # this is the forward step --> we calculate the new outputs based upon the input data from\n",
        "                # this batch and store the outputs in a variable\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # we compare the outputs to the ground truth labels in the batch to calculate the loss for this step\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                ## count the loss\n",
        "                losses[epoch,batch_index] = loss.item()\n",
        "\n",
        "                # we run backpropagation on the loss variable which repopulates the gradients all the way\n",
        "                # back through our model to the input layer\n",
        "                loss.backward()\n",
        "\n",
        "                # Use the gradients calculated in the backprop that took place in .backwards() to do a new\n",
        "                # gradient descent step\n",
        "                optimizer.step()\n",
        "            \n",
        "            ## After each epoch -- we should test validation accuracy\n",
        "            if do_validation:\n",
        "                if show_validation: print(\"Epoch: \", epoch)\n",
        "                validation_scores[epoch] = self.score(\"Validation\", print_score=show_validation)\n",
        "            \n",
        "                \n",
        "        ## Set Loss Matrix for visualizing\n",
        "        self.set_params(training_losses=losses)\n",
        "        self.set_params(validation_scores=validation_scores)\n",
        "        \n",
        "        return self\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Our PyTorch implementation of MLP\n",
        "from torch.nn import functional as F\n",
        "class MLP(torch.nn.Module):\n",
        "\n",
        "    ## the constructor is where we'll define all our layers (input, hidden, and output)\n",
        "    def __init__(self, hidden=50):\n",
        "\n",
        "        ## this line creates an instance of our parent (or base) class which in this case\n",
        "        ## is nn.Module.\n",
        "        super().__init__()\n",
        "\n",
        "        ## in the lines below we'll create instance variables and assign them torch.nn Models\n",
        "        ## in order to create our layers.  You should ordinarily have one variable definition for each layer\n",
        "        ## in your neural network except for the output layer.  The output layer is defined by the number of\n",
        "        ## outputs in your last layer. Since we're dealing with simple Artificial Neural Networks, we should\n",
        "        ## predominantly be using nn.Linear.  \n",
        "        \n",
        "        ## definee layer1\n",
        "        self.l1 = torch.nn.Linear(784, hidden)\n",
        "        \n",
        "        \n",
        "        # initialize layer1\n",
        "        torch.nn.init.xavier_uniform(self.l1.weight)\n",
        "        torch.nn.init.constant(self.l1.bias, 0.0)\n",
        "        \n",
        "        # define layer2\n",
        "        self.l2 = torch.nn.Linear(hidden,10)\n",
        "\n",
        " \n",
        "    # forwards takes as a parameter x -- the batch of inputs that we want to feed into our neural network model\n",
        "    # and returns the output of the model ... i.e. the results of the output layer of the model after forward\n",
        "    # propagation through our model. practically this means you should call each layer you defined in the\n",
        "    # constructor in sequence plus any activation functions on each layer.\n",
        "    def forward(self, x):\n",
        "     \n",
        "        # call all our layers on our input (in this case we only need one)\n",
        "        x = self.l1(x)\n",
        "        x = F.tanh(x)\n",
        "        x = self.l2(x)\n",
        "\n",
        "        # Since we're using Cross Entropy Loss\n",
        "        # we can return our output directly\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ok, lets run with a reg rate of 0.001, learning rate 0.1,batch size 100, 10 epochs, and 100 neurons in the hidden layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"MNIST MLP -- Hidden {}\".format(100))\n",
        "mnist_MLP= ANN(MLP,reg_rate=0.001,learning_rate=0.1,batch_size=100, hidden=100, epochs=10)\n",
        "mnist_MLP.fit()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_MLP.score(dataset=\"Validation\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_MLP.score(dataset=\"Test\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets change the number of hidden neurons between 50 to 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Lets try to find the optimal learning rate.**. Try [0.001, 0.01, 0.1, 0.5, 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Use the `optimal_learning_rate`  As noted earlier, batch size has less effect, but let's use a size of 100 anyway**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_MLP.score(dataset = \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "viz_training_loss(mnist_MLP.get_params('training_losses'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Great.  Test set accuracy of ~96%.  Let's try some different regularization rates.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ]
}